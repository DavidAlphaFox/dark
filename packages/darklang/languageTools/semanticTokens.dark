module Darklang =
  module LanguageTools =
    module SemanticTokens =
      // <aliases>
      type SourceRange = Parser.Range
      // <aliases>

      type TokenType =
        // universal
        | Symbol
        | Keyword

        // types
        | ModuleName
        | TypeName

        // exprs and fns
        | Operator
        | String
        | Number
        | ParameterName
        | VariableName
        | FunctionName

        // FUTURE: these are not yet used
        | Comment

        | TypeParameter

        | EnumCase // in Option.Some, this would be `Some`
        | Property // ? maybe this should be used for fields
        | Method // ? or maybe this should be used for fields
        | RegularExpression


      /// The LSP[1] communicates in terms of 'relative' semantic tokens,
      /// referring to 'deltas' of lines/characters since the previously-
      /// mentioned token. It's a bit easier to think about and map to 'exact'
      /// semantic tokens, though.
      ///
      /// [1] Semantic tokenization is currently only used for our language server,
      /// which follows the Language Server Protocol. That said, it may prove useful
      /// to tokenize for other reasons in the future and it's much easier to
      /// tokenize into an intermediate format rather than mapping directly to the
      /// data that the LSP expects.
      ///
      /// These tokens mean little without reference to a document where the
      /// 'ranges' live within.
      type SemanticToken =
        { sourceRange: SourceRange
          tokenType: TokenType }

      let makeToken (s: SourceRange) (t: TokenType) : SemanticToken =
        SemanticToken { sourceRange = s; tokenType = t }


      module ModuleIdentifier =
        let tokenize (m: WrittenTypes.ModuleIdentifier) : List<SemanticToken> =
          [ makeToken m.range TokenType.ModuleName ]

      module TypeIdentifier =
        let tokenize (t: WrittenTypes.TypeIdentifier) : List<SemanticToken> =
          [ makeToken t.range TokenType.TypeName ]

      module QualifiedTypeIdentifier =
        // Darklang.Stdlib.Option
        let tokenize
          (q: WrittenTypes.QualifiedTypeIdentifier)
          : List<SemanticToken> =
          [ // Darklang.Stdlib.
            (q.modules
             |> Stdlib.List.map (fun (m, _) -> ModuleIdentifier.tokenize m)
             |> Stdlib.List.flatten)

            // Option
            TypeIdentifier.tokenize q.typ ]
          |> Stdlib.List.flatten

      module VariableIdentifier =
        let tokenize (v: WrittenTypes.VariableIdentifier) : List<SemanticToken> =
          [ makeToken v.range TokenType.VariableName ]

      module FnIdentifier =
        let tokenize (fn: WrittenTypes.FnIdentifier) : List<SemanticToken> =
          [ makeToken fn.range TokenType.FunctionName ]

      module QualifiedFnIdentifier =
        /// Darklang.Stdlib.List.map
        let tokenize (q: WrittenTypes.QualifiedFnIdentifier) : List<SemanticToken> =
          [ // Darklang.Stdlib.List.
            (q.modules
             |> Stdlib.List.map (fun (m, _) -> ModuleIdentifier.tokenize m)
             |> Stdlib.List.flatten)

            // map
            FnIdentifier.tokenize q.fn ]
          |> Stdlib.List.flatten


      module TypeReference =
        module Builtin =
          let tokenize
            (t: WrittenTypes.TypeReference.Builtin)
            : List<SemanticToken> =
            match t with
            | TUnit r -> [ makeToken r TokenType.TypeName ]
            | TBool r -> [ makeToken r TokenType.TypeName ]
            | TInt8 r -> [ makeToken r TokenType.TypeName ]
            | TUInt8 r -> [ makeToken r TokenType.TypeName ]
            | TInt16 r -> [ makeToken r TokenType.TypeName ]
            | TUInt16 r -> [ makeToken r TokenType.TypeName ]
            | TInt32 r -> [ makeToken r TokenType.TypeName ]
            | TUInt32 r -> [ makeToken r TokenType.TypeName ]
            | TInt64 r -> [ makeToken r TokenType.TypeName ]
            | TUInt64 r -> [ makeToken r TokenType.TypeName ]
            | TInt128 r -> [ makeToken r TokenType.TypeName ]
            | TUInt128 r -> [ makeToken r TokenType.TypeName ]
            | TFloat r -> [ makeToken r TokenType.TypeName ]
            | TChar r -> [ makeToken r TokenType.TypeName ]
            | TString r -> [ makeToken r TokenType.TypeName ]
            | TList(r, rk, ro, t, rc) ->
              Stdlib.List.flatten (
                [ [ makeToken r TokenType.TypeName ]
                  [ makeToken rk TokenType.Keyword ]
                  [ makeToken ro TokenType.Symbol ]
                  (TypeReference.tokenize t)
                  [ makeToken rc TokenType.Symbol ] ]
              )
            | TDict(r, rk, ro, t, rc) ->
              Stdlib.List.flatten (
                [ [ makeToken r TokenType.TypeName ]
                  [ makeToken rk TokenType.Keyword ]
                  [ makeToken ro TokenType.Symbol ]
                  (TypeReference.tokenize t)
                  [ makeToken rc TokenType.Symbol ] ]
              )
            | TTuple(r, first, ra, second, rest, ro, rc) ->
              [ [ makeToken r TokenType.Symbol ]
                // (
                [ makeToken ro TokenType.Symbol ]
                // first
                TypeReference.tokenize first
                // *
                [ makeToken ra TokenType.Symbol ]
                // second
                TypeReference.tokenize second
                // rest
                (rest
                 |> Stdlib.List.map (fun (symbolAsterisk, typ) ->
                   [ makeToken symbolAsterisk TokenType.Symbol ]
                   TypeReference.tokenize typ)
                 |> Stdlib.List.flatten)
                // )
                [ makeToken rc TokenType.Symbol ] ]
              |> Stdlib.List.flatten


        let tokenize
          (t: WrittenTypes.TypeReference.TypeReference)
          : List<SemanticToken> =
          match t with
          | Builtin b -> Builtin.tokenize b
          | QualifiedName q -> QualifiedTypeIdentifier.tokenize q


      module TypeDeclaration =
        let tokenizeDefinition
          (d: WrittenTypes.TypeDeclaration.Definition)
          : List<SemanticToken> =
          match d with
          | Alias typeRef -> TypeReference.tokenize typeRef
          | Record fields ->
            fields
            |> Stdlib.List.map (fun rf ->
              let (nameRange, _) = rf.name

              [ [ makeToken nameRange TokenType.Property ]
                [ makeToken rf.symbolColon TokenType.Symbol ]
                TypeReference.tokenize rf.typ ]
              |> Stdlib.List.flatten)
            |> Stdlib.List.flatten

        // type ID = UInt64
        let tokenize
          (t: WrittenTypes.TypeDeclaration.TypeDeclaration)
          : List<SemanticToken> =
          [ // type
            [ makeToken t.keywordType TokenType.Keyword ]
            // ID
            [ makeToken t.name.range TokenType.TypeName ]
            // =
            [ makeToken t.symbolEquals TokenType.Symbol ]
            // UInt64
            tokenizeDefinition t.definition ]
          |> Stdlib.List.flatten



      module Expr =
        module LetPattern =
          let tokenize (lp: WrittenTypes.LetPattern) : List<SemanticToken> =
            match lp with
            | LPVariable(range, _name) -> [ makeToken range TokenType.VariableName ]


        let tokenize (e: WrittenTypes.Expr) : List<SemanticToken> =
          match e with
          // ()
          | EUnit range -> [ makeToken range TokenType.Symbol ]

          // true
          | EBool(range, _b) -> [ makeToken range TokenType.Keyword ]

          // 12y
          | EInt8(_, (intPartRange, _), suffixPartRange) ->
            [ makeToken intPartRange TokenType.Number
              makeToken suffixPartRange TokenType.Symbol ]

          // 12uy
          | EUInt8(_, (intPartRange, _), suffixPartRange) ->
            [ makeToken intPartRange TokenType.Number
              makeToken suffixPartRange TokenType.Symbol ]

          // 12s
          | EInt16(_, (intPartRange, _), suffixPartRange) ->
            [ makeToken intPartRange TokenType.Number
              makeToken suffixPartRange TokenType.Symbol ]

          // 12us
          | EUInt16(_, (intPartRange, _), suffixPartRange) ->
            [ makeToken intPartRange TokenType.Number
              makeToken suffixPartRange TokenType.Symbol ]

          // 12l
          | EInt32(_, (intPartRange, _), suffixPartRange) ->
            [ makeToken intPartRange TokenType.Number
              makeToken suffixPartRange TokenType.Symbol ]

          // 12ul
          | EUInt32(_, (intPartRange, _), suffixPartRange) ->
            [ makeToken intPartRange TokenType.Number
              makeToken suffixPartRange TokenType.Symbol ]

          // 12L
          | EInt64(_, (intPartRange, _), suffixPartRange) ->
            [ makeToken intPartRange TokenType.Number
              makeToken suffixPartRange TokenType.Symbol ]

          // 12UL
          | EUInt64(_, (intPartRange, _), suffixPartRange) ->
            [ makeToken intPartRange TokenType.Number
              makeToken suffixPartRange TokenType.Symbol ]

          // 12Q
          | EInt128(_, (intPartRange, _), suffixPartRange) ->
            [ makeToken intPartRange TokenType.Number
              makeToken suffixPartRange TokenType.Symbol ]

          // 12Z
          | EUInt128(_, (intPartRange, _), suffixPartRange) ->
            [ makeToken intPartRange TokenType.Number
              makeToken suffixPartRange TokenType.Symbol ]

          // 12.0
          | EFloat(range, _sign, _whole, _fraction) ->
            [ makeToken range TokenType.Number ]

          // "hello"
          | EString(range, _contentsMaybe, symbolOpenQuote, symbolCloseQuote) ->
            [ // "
              [ makeToken symbolOpenQuote TokenType.Symbol ]

              // hello
              [ makeToken range TokenType.String ]

              // "
              [ makeToken symbolCloseQuote TokenType.Symbol ] ]
            |> Stdlib.List.flatten

          // 'a'
          | EChar(range, _contentsMaybe, symbolOpenQuote, symbolCloseQuote) ->
            [ // '
              [ makeToken symbolOpenQuote TokenType.Symbol ]

              // a
              [ makeToken range TokenType.String ]

              // '
              [ makeToken symbolCloseQuote TokenType.Symbol ] ]
            |> Stdlib.List.flatten

          | EList(range, contentsMaybe, symbolLeftBracket, symbolRightBracket) ->
            let contents =
              contentsMaybe
              |> Stdlib.List.map (fun (expr, _) -> Expr.tokenize expr)
              |> Stdlib.List.flatten

            [ // [
              [ makeToken symbolLeftBracket TokenType.Symbol ]
              // 1 ; 2 ; 3
              contents
              // ]
              [ makeToken symbolRightBracket TokenType.Symbol ] ]
            |> Stdlib.List.flatten

          | EDict(range,
                  contentsMaybe,
                  keywordDict,
                  symbolOpenBrace,
                  symbolCloseBrace) ->
            let contents =
              contentsMaybe
              |> Stdlib.List.map (fun (_, key, value) -> Expr.tokenize value)
              |> Stdlib.List.flatten

            [ // Dict
              [ makeToken keywordDict TokenType.Keyword ]
              // {
              [ makeToken symbolOpenBrace TokenType.Symbol ]
              // a = 1 ; b = 2 ; c = 3
              contents
              // }
              [ makeToken symbolCloseBrace TokenType.Symbol ] ]
            |> Stdlib.List.flatten

          // (1, 2, 3)
          | ETuple(range,
                   first,
                   symbolComma,
                   second,
                   rest,
                   symbolOpenParen,
                   symbolCloseParen) ->
            let rest =
              rest
              |> Stdlib.List.map (fun (_, expr) -> Expr.tokenize expr)
              |> Stdlib.List.flatten

            [ // (
              [ makeToken symbolOpenParen TokenType.Symbol ]
              // 1
              Expr.tokenize first
              // ,
              [ makeToken symbolComma TokenType.Symbol ]
              // 2
              Expr.tokenize second
              // , 3
              rest
              // )
              [ makeToken symbolCloseParen TokenType.Symbol ] ]
            |> Stdlib.List.flatten

          // Person { name = "Alice" }
          | ERecord(range, typeName, fields, symbolOpenBrace, symbolCloseBrace) ->
            let typeName =
              match typeName with
              | Unresolved(range, _) -> [ makeToken range TokenType.TypeName ]
              | _ -> []

            let fields =
              fields
              |> Stdlib.List.map (fun (symbol, fieldName, value) ->
                // TODO: use tuple destructuring, once nested tuple destructuring in a lambda is fixed
                // i.e. fun (symbol, (sourceRange, _), value) -> ... instead of the below code
                let (range, _) = fieldName

                [ // name
                  [ makeToken range TokenType.Property ]
                  // =
                  [ makeToken symbol.sourceRange TokenType.Symbol ]
                  // "Alice"
                  Expr.tokenize value ])
              |> Stdlib.List.flatten

            [ // Person
              typeName
              // {
              [ makeToken symbolOpenBrace TokenType.Symbol ]
              // name = "Alice"
              fields |> Stdlib.List.flatten
              // }
              [ makeToken symbolCloseBrace TokenType.Symbol ] ]
            |> Stdlib.List.flatten

          // let x = 2
          // x + 1
          | ELet(range, lp, expr, body, keywordLet, symbolEquals) ->
            [ [ makeToken keywordLet TokenType.Keyword ]
              LetPattern.tokenize lp
              [ makeToken symbolEquals TokenType.Symbol ]
              Expr.tokenize expr
              Expr.tokenize body ]
            |> Stdlib.List.flatten

          // x
          | EVariable(range, _varName) -> [ makeToken range TokenType.VariableName ]

          // if true then 1 else 2
          | EIf(range, cond, thenExpr, elseExpr, keywordIf, keywordThen, keywordElse) ->
            let elseKeyword =
              Stdlib.Option.mapWithDefault keywordElse [] (fun e ->
                [ makeToken e TokenType.Keyword ])

            let elseExpr =
              Stdlib.Option.mapWithDefault elseExpr [] (fun e -> Expr.tokenize e)

            [ // if
              [ makeToken keywordIf TokenType.Keyword ]
              // true
              Expr.tokenize cond
              // then
              [ makeToken keywordThen TokenType.Keyword ]
              // 1
              Expr.tokenize thenExpr
              // else
              elseKeyword
              // 2
              elseExpr ]
            |> Stdlib.List.flatten

          // x + 1
          | EInfix(_, (opRange, _op), left, right) ->
            [ // x
              Expr.tokenize left
              // +
              [ makeToken opRange TokenType.Operator ]
              // 1
              Expr.tokenize right ]
            |> Stdlib.List.flatten

          // hacky temp. syntax -- see `grammar.js`
          // (Int64.add 1L 2L)
          | EFnCall(range, fnName, args, symbolLeftParen, symbolRightParen) ->
            [ // (
              [ makeToken symbolLeftParen TokenType.Symbol ]
              // Int64.add
              QualifiedFnIdentifier.tokenize fnName
              // 1L 2L
              (args
               |> Stdlib.List.map (fun arg -> Expr.tokenize arg)
               |> Stdlib.List.flatten)
              // )
              [ makeToken symbolRightParen TokenType.Symbol ] ]
            |> Stdlib.List.flatten



      module FnDeclaration =
        module UnitParameter =
          // `()`
          let tokenize
            (p: WrittenTypes.FnDeclaration.UnitParameter)
            : List<SemanticToken> =
            [ makeToken p.range TokenType.Symbol ]

        // `(a: Int64)`
        module NormalParameter =
          let tokenize
            (p: WrittenTypes.FnDeclaration.NormalParameter)
            : List<SemanticToken> =
            [ // `(`
              [ makeToken p.symbolLeftParen TokenType.Symbol ]
              // `a`
              [ makeToken p.name.range TokenType.ParameterName ]
              // `:`
              [ makeToken p.symbolColon TokenType.Symbol ]
              // `Int64`
              TypeReference.tokenize p.typ
              // `)`
              [ makeToken p.symbolRightParen TokenType.Symbol ] ]
            |> Stdlib.List.flatten

        module Parameter =
          let tokenize
            (p: WrittenTypes.FnDeclaration.Parameter)
            : List<SemanticToken> =
            match p with
            | Unit u -> UnitParameter.tokenize u
            | Normal n -> NormalParameter.tokenize n


        // `let add (a: Int64) (b: Int64): Int64 = a + b`
        let tokenize
          (fn: WrittenTypes.FnDeclaration.FnDeclaration)
          : List<SemanticToken> =
          [ // `let`
            [ makeToken fn.keywordLet TokenType.Keyword ]
            // `add`
            FnIdentifier.tokenize fn.name
            // `(a: Int64) (b: Int64)`
            (fn.parameters
             |> Stdlib.List.map (fun p -> Parameter.tokenize p)
             |> Stdlib.List.flatten)
            // `:`
            [ makeToken fn.symbolColon TokenType.Symbol ]
            // `Int64`
            TypeReference.tokenize fn.returnType
            // `=`
            [ makeToken fn.symbolEquals TokenType.Operator ]
            // `a + b`
            Expr.tokenize fn.body ]
          |> Stdlib.List.flatten


      module ParsedFile =
        let tokenize (wt: WrittenTypes.ParsedFile) : List<SemanticToken> =
          match wt with
          | CliScript cliScript ->
            let typesAndFnsPart =
              cliScript.typesAndFns
              |> Stdlib.List.map (fun typeOrFn ->
                match typeOrFn with
                | Type t -> TypeDeclaration.tokenize t
                | Function fn -> FnDeclaration.tokenize fn)
              |> Stdlib.List.flatten

            let exprsPart =
              cliScript.exprsToEval
              |> Stdlib.List.map (fun expr -> Expr.tokenize expr)
              |> Stdlib.List.flatten

            Stdlib.List.flatten [ typesAndFnsPart; exprsPart ]