apiVersion: apps/v1
kind: Deployment
metadata:
  name: legacyserver-deployment
  namespace: darklang
  annotations:
    kubernetes.io/change-cause: "{ARG:CHANGE_CAUSE}"

spec:
  revisionHistoryLimit: 10
  replicas: 60
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1

  selector:
    matchLabels:
      app: legacyserver-app
  template:
    metadata:
      labels:
        app: legacyserver-app
    spec:
      # ---------------------
      # Pod-level security
      # ---------------------
      securityContext:
        seccompProfile:
          type: RuntimeDefault
        runAsNonRoot: true # https://docs.bridgecrew.io/docs/bc_k8s_22
        runAsUser: 25432 # https://stackoverflow.com/questions/49720308
      hostPID: false
      automountServiceAccountToken: false # https://docs.bridgecrew.io/docs/bc_k8s_35

      ###################
      # Container definitions
      ###################
      containers:
        #########################
        # Legacyserver container
        #########################
        - name: legacy-ctr
          image: "gcr.io/balmy-ground-195100/legacyserver:{IMAGEID:legacyserver}"
          ports:
            - name: legacy-port
              containerPort: 5000

          # ---------------------
          # Security - https://docs.bridgecrew.io/docs
          # ---------------------
          imagePullPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - NET_RAW # https://docs.bridgecrew.io/docs/bc_k8s_27
            privileged: false
            # this is expected to run as root. We're getting rid of the container
            # soon so not worth fixing.
            runAsNonRoot: false
            runAsUser: 0

          # ---------------------
          # Resource limits
          # ---------------------
          # Resource limits + requests are intentionally the same, to ensure
          # this pod is a 'Guaranteed' pod, ref:
          # https://medium.com/google-cloud/quality-of-service-class-qos-in-kubernetes-bb76a89eb2c6
          resources:
            requests:
              memory: "100Mi"
              cpu: "10m"
            limits:
              memory: "1000Mi"
              cpu: "125m"

          # ---------------------
          # Lifecycle probes
          # ---------------------
          lifecycle:
            preStop:
              httpGet:
                # ???? https://github.com/kubernetes/kubernetes/issues/56770
                path: /k8s/pkill
                port: legacy-port
          readinessProbe:
            httpGet:
              path: /k8s/readinessProbe
              port: legacy-port
            initialDelaySeconds: 5
            periodSeconds: 5
          livenessProbe:
            httpGet:
              path: /k8s/livenessProbe
              port: legacy-port
            # Giving 2 minutes grace here, there's an outstanding k8s issue
            # preventing you from specifying "start checking liveness after an
            # ok from readiness", which is what you'd expect.
            # https://github.com/kubernetes/kubernetes/issues/27114
            initialDelaySeconds: 120
            periodSeconds: 10 # every 10 seconds
            timeoutSeconds: 10 # time out after 10 seconds
            failureThreshold: 3 # kill container after 3 successive time outs

          # ---------------------
          # Environment
          # ---------------------
          envFrom:
            - configMapRef:
                name: "{VERSIONED-CONFIGMAP:app-config}"

      # ---------------------
      # Pod volume definitions
      # ---------------------
      volumes:
        # to allow readOnlyRootFilesystem
        - name: rundir
          emptyDir: { }
