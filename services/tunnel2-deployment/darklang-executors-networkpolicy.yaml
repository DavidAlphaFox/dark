# Pods that do dark execution should all have the same set of rules for what they can
# access, specifically that they all route their traffic through the tunnel2 service
# These pods are labelled with `dark-executor: "true"`

# Egress to allow their traffic into tunnel2, as well as other places they need to go
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: dark-executors
  namespace: darklang
spec:
  policyTypes:
    - Egress
  podSelector:
    matchLabels:
      dark-executor: "true"
  egress:
    # Allow access to kube-dns so that dns discovery of tunnel2 works
    - to:
        - namespaceSelector:
            matchLabels:
              k8s-namespace: kube-system
          podSelector:
            matchExpressions:
              - key: k8s-app
                operator: In
                values: ["kube-dns", "node-local-dns"]
      ports:
        - protocol: TCP
          port: 53
        - protocol: UDP
          port: 53

    # Allow access to tunnel2
    - to:
        - namespaceSelector:
            matchLabels:
              k8s-namespace: darklang
          podSelector:
            matchLabels:
              app: tunnel2-app
      ports:
        - protocol: TCP
          port: 1080

    # Allow access to Cloud SQL
    - to:
        - ipBlock:
            cidr: 35.203.144.131/32
      ports:
        - protocol: TCP
          port: 3307 # port cloudsqlproxy uses
        # Cloud sql proxy needs to hit a JSON endpoint to get the settings
        - protocol: TCP
          port: 443
        - protocol: UDP
          port: 53

    # Allow access to Cloud SQL proxy: TODO:
    - to:
        - ipBlock:
            cidr: 127.0.0.1/32
      ports:
        - protocol: TCP
          port: 5432 # port cloudsqlproxy provides
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 443

    # Accessing the DB isn't working with the above settings, so allow access to
    # everywhere while we figure this out. This isn't as secure as it could be, but
    # it's not terrible either considering that the outbound traffic is going through
    # tunnel2, and we've locked down internal stuff.
    # TODO: it seems that the extra access is to sqladmin.googleapis.com, which is
    # not an explicit IP and so can't be tackled in the rules here. Recently it has
    # been both 142.251.40.138 and 172.217.165.138. I have a support request in with
    # GCP to see what to do here.
    - to:
        - ipBlock:
            cidr: 0.0.0.0/0
            except:
              # Exclude any local addresses. We allow some of these above, but rules
              # are ORed together, so it's fine to exclude them here.
              - 169.254.0.0/16
              - 10.0.0.0/8
              - 172.16.0.0/12
              - 192.168.0.0/16
              - 127.0.0.0/8
      ports:
        - port: 443
          protocol: TCP
        - port: 53
          protocol: UDP
