## gzip makes responses much smaller
gzip on;

## nginx assumes proxies can't handle gzip. That's wrong in our case;
## the gke load-balancer will handle it fine, and in fact needs a gzipped
## response to gzip.
## http://nginx.org/en/docs/http/ngx_http_gzip_module.html#gzip_proxied
gzip_proxied any;

## gzip these mime types. some other types of files are already gzipped
## in a content-aware way (e.g. png, jpeg) so it probably doesn't make
## sense to re-gzip them. (text/html isn't in this configuration file
## because it's always there, and including it makes nginx warn. )
gzip_types text/plain text/css application/javascript application/json;

# don't gzip small files.
gzip_min_length 1024;

proxy_cache_path /tmp/cache/ levels=1:2 keys_zone=static_cache:100k max_size=100m;

# log_format for honeycomb/honeytail: https://docs.honeycomb.io/getting-data-in/integrations/webservers/nginx/#optional-configuration
# added: x-darklang-execution-id
# added: cookie (so we know which user did a thing)
log_format honeycomb '$remote_addr - $remote_user [$time_local] $host '
    '"$request" $status $bytes_sent $body_bytes_sent $request_time '
    '"$http_referer" "$http_user_agent" $request_length "$http_authorization" '
    '"$http_x_forwarded_proto" "$http_x_forwarded_for" $server_name '
    '"$upstream_http_x_darklang_execution_id" "$http_cookie" "$upstream_http_x_dark_username" "$http_x_darklang_client_version" "$upstream_http_x_darklang_server_version"';
access_log /var/log/nginx/access.log honeycomb;

# 'trust' all ips, rather than the footgun of "oops, changed our incoming ip,
# forgot to update nginx". We're using this remote_addr value for stats, not
# auth, so untrusted is ok.
set_real_ip_from 0.0.0.0/0;
set_real_ip_from ::/0;

real_ip_header X-Forwarded-For;
real_ip_recursive on;

# Tune nginx keepalives to work with the GCP HTTP(S) Load Balancer, per
# https://blog.percy.io/tuning-nginx-behind-google-cloud-platform-http-s-load-balancer-305982ddb340
keepalive_timeout 650;
keepalive_requests 10000;

# Redirect www.darklang.com to darklang.com
server {
  listen 9000;
  server_name www.darklang.com;

  # tell clients to stop going to http://www.darklang.com
  add_header Strict-Transport-Security "max-age=31536000; includeSubDomains; preload" always;

  return 301 https://darklang.com$request_uri;
}

# static.darklang, for dev-mode only
server {
  # In dev-mode only, we support static.darklang in the apiserver. In production,
  # static.darklang.com points to a different IP address.
  listen 9000;
  server_name static.darklang.localhost;

  location / {
    proxy_pass http://localhost:9001;
  }
}

# Main darklang.com behaviour
server {
  listen 9000;
  server_name darklang.com darklang.localhost;

  # At least in theory, we could put client_max_body_size under a location {},
  # so we could add a location /api/*/static_assets {}, to leave the default of
  # 1m in place elsewhere. Stack overflow disagrees; unclear whether this may be
  # due to different nginx versions. So for now, doing it more broadly to be
  # safe.
  client_max_body_size 100m;

  # tell clients to stop going to http://darklang.com
  add_header Strict-Transport-Security "max-age=31536000; includeSubDomains; preload" always;

  # These prefixes are handled by the backend.
  location ~ (/a/|/api/|/login|/logout|/check-apiserver) {
    # https://docs.microsoft.com/en-us/aspnet/core/host-and-deploy/linux-nginx?view=aspnetcore-6.0#configure-nginx
    proxy_pass         http://localhost:9001;
    proxy_http_version 1.1;
    proxy_set_header   Upgrade $http_upgrade;
    proxy_set_header   Connection keep-alive;
    proxy_set_header   Host $host;
    proxy_cache_bypass $http_upgrade;
    # Don't set x-forwarded-for or x-forwarded-proto, as we want to get those from the upstream GCP proxy
  }

  # The rest of the routes should proxy to the marketing site.
  location / {
    # redirect http to https.
    if ($http_x_forwarded_proto = "http") {
      rewrite ^(.*)$ https://$server_name$1 permanent;
    }

    proxy_set_header X-Forwarded-Host $host;
    proxy_pass https://ops-corpsite.builtwithdark.com;
  }
}

# Healthcheck for the GCE ingress
server {
  # We don't want this to trigger for actual dark traffic. It will trigger if someone
  # hits the ingress with its IP address, which is a fine, security wise.
  server_name "";

  # The load balancer sends this request on post 80 which comes to 9000 (I tried
  # other ports but couldn't make it work pointing to port 9002).
  listen 9000;

  # The GCE load balancer must pass before a node is added to the load balancer. This
  # prevents pods on that node from being added when they're not live. Since the
  # apiserver or other parts of the pod could have liveness issues (for example if
  # they restarts) it makes sense to pass the check onto the apiserver container,
  # rather than answering the liveness poll here.
  location /k8s/livenessProbe {
    proxy_pass         http://localhost:9002;
    proxy_http_version 1.1;
  }
}
